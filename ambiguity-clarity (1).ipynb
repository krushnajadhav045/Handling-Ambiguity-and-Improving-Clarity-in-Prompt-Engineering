{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Ambiguity and Improving Clarity in Prompt Engineering\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial focuses on two critical aspects of prompt engineering: identifying and resolving ambiguous prompts, and techniques for writing clearer prompts. These skills are essential for effective communication with AI models and obtaining more accurate and relevant responses.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Ambiguity in prompts can lead to inconsistent or irrelevant AI responses, while lack of clarity can result in misunderstandings and inaccurate outputs. By mastering these aspects of prompt engineering, you can significantly improve the quality and reliability of AI-generated content across various applications.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Identifying ambiguous prompts\n",
    "2. Strategies for resolving ambiguity\n",
    "3. Techniques for writing clearer prompts\n",
    "4. Practical examples and exercises\n",
    "\n",
    "## Method Details\n",
    "\n",
    "We'll use OpenAI's GPT or any model  model and the LangChain library to demonstrate various techniques for handling ambiguity and improving clarity in prompts. The tutorial will cover:\n",
    "\n",
    "1. Setting up the environment and necessary libraries\n",
    "2. Analyzing ambiguous prompts and their potential interpretations\n",
    "3. Implementing strategies to resolve ambiguity, such as providing context and specifying parameters\n",
    "4. Exploring techniques for writing clearer prompts, including using specific language and structured formats\n",
    "5. Practical exercises to apply these concepts in real-world scenarios\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this tutorial, you'll have a solid understanding of how to identify and resolve ambiguity in prompts, as well as techniques for crafting clearer prompts. These skills will enable you to communicate more effectively with AI models, resulting in more accurate and relevant outputs across various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# # Load environment variables\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# # Set up OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# # Initialize the language model\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OpenAI (paid)\n",
    "\n",
    "- GeminAI (free for limited time)\n",
    "\n",
    "- We can use llama model using GroQ key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api=\"API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature (float, 0–2, default = 1.0)\n",
    "\n",
    "- Controls randomness of the output.\n",
    "\n",
    "- Lower = more deterministic, higher = more creative.\n",
    "\n",
    "- 0.0 → nearly deterministic\n",
    "\n",
    "- 0.7 → balanced (often used)\n",
    "\n",
    "- >1.2 → very diverse/creative but may lose coherence\n",
    "\n",
    "## max_tokens (int)\n",
    "\n",
    "- Sets the maximum number of tokens in the generated response.\n",
    "\n",
    "- Doesn’t affect input length — only the completion output.\n",
    "\n",
    "- Example: max_tokens=512 will never generate more than 512 tokens.\n",
    "\n",
    "## top_p (float, 0–1, default = 1.0)\n",
    "\n",
    "- Enables nucleus sampling.\n",
    "\n",
    "- Instead of sampling from all tokens, the model only considers the smallest set of tokens whose cumulative probability ≥ top_p.\n",
    "\n",
    "- Lower values = more focused, higher values = more diverse.\n",
    "\n",
    "- 0.9 → common choice, cuts off rare low-probability words\n",
    "\n",
    "## top_k (int, default varies by model)\n",
    "\n",
    "- Enables top-k sampling.\n",
    "\n",
    "- The model only considers the k most likely next tokens before sampling.\n",
    "\n",
    "- top_k=1 → greedy decoding (always picks most likely token)\n",
    "\n",
    "- top_k=50 → sample from top 50 candidates\n",
    "\n",
    "- Can be used together with top_p (model applies both filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-3.3-70b-versatile\n",
    "\n",
    "**Context Window & Output Length**\n",
    "\n",
    "- According to Groq’s documentation, the model supports a context window of 131,072 tokens, which means it can process this much input (including prompt and response) in one go \n",
    "\n",
    "- The maximum output tokens (completion length) is 32,768 tokens \n",
    "\n",
    "\n",
    "- So effectively:\n",
    "\n",
    "    - Max Input (prompt + response): 131,072 tokens\n",
    "\n",
    "    - Max Output (completion): 32,768 tokens\n",
    "\n",
    "- Vocabulary Size\n",
    "\n",
    "| Specification         | Value                                                  |\n",
    "| --------------------- | ------------------------------------------------------ |\n",
    "| **Context Window**    | 131,072 tokens                                         |\n",
    "| **Max Output Tokens** | 32,768 tokens                                          |\n",
    "| **Vocabulary Size**   | Not officially stated; Llama 3.1 used \\~128,000 tokens |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002002F2E4590>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002002F2E4FB0>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=512)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=groq_api,\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Ambiguous Prompts\n",
    "\n",
    "Let's start by examining some ambiguous prompts and analyzing their potential interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me about the bank.\n",
      "The prompt \"Tell me about the bank\" is ambiguous because the word \"bank\" has multiple meanings, and the context is not specified. This leads to several possible interpretations:\n",
      "\n",
      "1. **Financial institution**: The most common association with \"bank\" is a financial institution where people deposit and withdraw money, such as a commercial bank or a central bank. In this context, the prompt might be asking for information about a specific bank's services, history, or location.\n",
      "2. **Riverbank or lake bank**: A \"bank\" can also refer to the land alongside a river, lake, or sea. The prompt might be asking about the geography, geology, or features of a specific riverbank or lake bank.\n",
      "3. **Storage location**: In some contexts, a \"bank\" can refer to a storage location for specific items, such as a blood bank (for storing blood donations) or a seed bank (for storing plant seeds).\n",
      "4. **Slope or incline**: In aviation, a \"bank\" refers to the tilt of an aircraft during a turn. In this context, the prompt might be asking about the mechanics of banking in flight.\n",
      "5. **Data bank or database**: A \"bank\" can also refer to a collection of data or information, such as a database or a repository of knowledge.\n",
      "\n",
      "The ambiguity arises from the lack of context, which makes it difficult to determine which interpretation is intended. To clarify the prompt, additional information would be needed, such as:\n",
      "\n",
      "* \"Tell me about the bank where I can deposit my money.\"\n",
      "* \"Tell me about the bank of the river that runs through our city.\"\n",
      "* \"Tell me about the blood bank at the hospital.\"\n",
      "\n",
      "Without additional context, it's challenging to provide a specific answer, and the prompt remains open to multiple interpretations.\n",
      "--------------------------------------------------\n",
      "Prompt: What's the best way to get to school?\n",
      "The prompt \"What's the best way to get to school?\" is ambiguous because it lacks specific details and context, leading to multiple possible interpretations. Here are some reasons why it's ambiguous and potential interpretations:\n",
      "\n",
      "1. **Undefined location**: The prompt doesn't specify the location of the school or the starting point of the journey. Different locations may have varying transportation options, making it difficult to provide a universally applicable answer.\n",
      "2. **Unspecified mode of transportation**: The prompt doesn't mention whether the person is looking for a walking, driving, cycling, or public transportation route. Each mode has its own set of considerations, such as traffic, safety, and convenience.\n",
      "3. **Personal preferences and priorities**: The term \"best\" is subjective and can depend on individual preferences, such as:\n",
      "\t* Fastest route\n",
      "\t* Most affordable option\n",
      "\t* Safest route\n",
      "\t* Most environmentally friendly option\n",
      "\t* Route with the least traffic or congestion\n",
      "4. **Assumed audience**: The prompt doesn't specify who is asking the question, which can affect the type of response. For example:\n",
      "\t* A student might be looking for a route that is convenient and affordable.\n",
      "\t* A parent might prioritize safety and reliability.\n",
      "\t* A school administrator might focus on routes that promote sustainability or reduce traffic congestion.\n",
      "5. **Time and schedule constraints**: The prompt doesn't account for time-sensitive factors, such as:\n",
      "\t* Rush hour traffic\n",
      "\t* School bell schedules\n",
      "\t* Bus or train schedules\n",
      "\n",
      "Possible interpretations of the prompt include:\n",
      "\n",
      "* What is the quickest way to get to school?\n",
      "* What is the most affordable way to get to school?\n",
      "* What is the safest route to school?\n",
      "* What is the most environmentally friendly way to get to school?\n",
      "* What is the best way to get to school during peak traffic hours?\n",
      "* What is the most convenient way to get to school for a student with limited mobility?\n",
      "\n",
      "To clarify the prompt and provide a more accurate response, additional information would be necessary, such as the location of the school, the starting point of the journey, and the individual's priorities and preferences.\n",
      "--------------------------------------------------\n",
      "Prompt: Can you explain the theory?\n",
      "The prompt \"Can you explain the theory?\" is ambiguous because it lacks specificity and context. Here's why:\n",
      "\n",
      "1. **Unclear referent**: The word \"theory\" is not specified, leaving the reader wondering which theory is being referred to. There are countless theories across various fields, such as physics, biology, psychology, economics, and more.\n",
      "2. **Lack of context**: The prompt doesn't provide any context about the topic, field, or discipline related to the theory. This makes it difficult to determine the scope and relevance of the theory.\n",
      "3. **Ambiguous request**: The phrase \"explain the theory\" is vague. Does the speaker want a brief summary, a detailed analysis, or a comprehensive overview of the theory?\n",
      "\n",
      "Possible interpretations of the prompt include:\n",
      "\n",
      "* **Request for a general explanation**: The speaker might be asking for a basic overview of a specific theory, such as the theory of relativity or the theory of evolution.\n",
      "* **Request for a detailed analysis**: The speaker might be seeking an in-depth examination of a theory, including its underlying assumptions, key concepts, and implications.\n",
      "* **Request for a comparison or critique**: The speaker might be asking for a comparison of different theories or a critical evaluation of a particular theory.\n",
      "* **Request for a specific application**: The speaker might be interested in understanding how a theory applies to a particular problem, industry, or field.\n",
      "* **Request for a definition or clarification**: The speaker might be seeking a clear definition of a theory or clarification on a specific aspect of the theory.\n",
      "\n",
      "To disambiguate the prompt, it would be helpful to provide more context, such as:\n",
      "\n",
      "* Specifying the theory or field of study (e.g., \"Can you explain the theory of gravity?\" or \"Can you explain the theory of cognitive development in psychology?\")\n",
      "* Providing background information or context (e.g., \"In the context of climate change, can you explain the theory of global warming?\")\n",
      "* Clarifying the level of detail or analysis desired (e.g., \"Can you provide a brief summary of the theory?\" or \"Can you provide a detailed analysis of the theory, including its strengths and limitations?\")\n",
      "\n",
      "By adding more specificity and context, the prompt can be made more clear and concise, allowing for a more accurate and helpful response.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ambiguous_prompts = [\n",
    "    \"Tell me about the bank.\",\n",
    "    \"What's the best way to get to school?\",\n",
    "    \"Can you explain the theory?\"\n",
    "]\n",
    "\n",
    "for prompt in ambiguous_prompts:\n",
    "    analysis_prompt = f\"Analyze the following prompt for ambiguity: '{prompt}'. Explain why it's ambiguous and list possible interpretations.\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(llm.invoke(analysis_prompt).content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving Ambiguity\n",
    "\n",
    "Now, let's explore strategies for resolving ambiguity in prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: You are a financial advisor discussing savings accounts.\n",
      "Clarified response: The bank I'm affiliated with is a well-established and reputable institution that offers a wide range of financial services, including savings accounts. We have a long history of providing our customers with secure and convenient ways to manage their money.\n",
      "\n",
      "Our bank is committed to helping individuals and families achieve their financial goals, whether that's saving for a specific purpose, such as a down payment on a house, or simply building up an emergency fund. We offer a variety of savings account options, each with its own unique features and benefits.\n",
      "\n",
      "Some of the key features of our savings accounts include competitive interest rates, low or no monthly maintenance fees, and easy access to your money through online banking, mobile banking, or in-person at one of our many branch locations. We also offer overdraft protection, which can help you avoid unexpected overdraft fees.\n",
      "\n",
      "In addition to our savings accounts, our bank also offers a range of other financial products and services, including checking accounts, loans, credit cards, and investment products. We have a team of experienced financial advisors, including myself, who can help you navigate the different options and create a personalized plan to achieve your financial goals.\n",
      "\n",
      "Overall, our bank is dedicated to providing exceptional customer service, competitive rates, and a wide range of financial solutions to help our customers succeed. Whether you're just starting to save or are looking for more advanced financial planning, we're here to help. Would you like to learn more about our savings account options or discuss your specific financial goals?\n",
      "--------------------------------------------------\n",
      "Context: You are a geographer describing river formations.\n",
      "Clarified response: The bank of a river is a crucial component of its formation and morphology. In geography, a bank refers to the edge or margin of a river, where the water meets the land. It's the area where the river's channel meets the surrounding terrain, and it plays a vital role in shaping the river's course and behavior.\n",
      "\n",
      "There are two types of banks: the cut bank and the point bar bank. The cut bank is the outside bank of a meandering river, where the water erodes the soil and rocks, causing the bank to retreat. This process is known as lateral erosion. On the other hand, the point bar bank is the inside bank of a meandering river, where sediment is deposited, causing the bank to build up.\n",
      "\n",
      "The bank of a river can be composed of various materials, including soil, sand, gravel, and rocks. The type and stability of the bank material can influence the river's flow, erosion, and deposition patterns. For example, a river with a bank composed of soft, erodible materials like sand or silt may be more prone to meandering and changing its course over time.\n",
      "\n",
      "The bank also provides a habitat for various plant and animal species, such as riparian vegetation, fish, and other aquatic organisms. The bank's slope, curvature, and vegetation can affect the river's hydrology, sediment transport, and water quality.\n",
      "\n",
      "In addition, human activities like river engineering, agriculture, and urban development can alter the bank of a river, leading to changes in its morphology, hydrology, and ecology. Understanding the bank of a river is essential for managing river systems, mitigating the impacts of human activities, and preserving the natural functions and benefits of rivers.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def resolve_ambiguity(prompt, context):\n",
    "    \"\"\"\n",
    "    Resolve ambiguity in a prompt by providing additional context.\n",
    "    \"\"\"\n",
    "    clarified_prompt = f\"{context}\\n\\nBased on this context, {prompt}\"\n",
    "    return llm.invoke(clarified_prompt).content\n",
    "\n",
    "# Example usage\n",
    "ambiguous_prompt = \"Tell me about the bank.\"\n",
    "contexts = [\n",
    "    \"You are a financial advisor discussing savings accounts.\",\n",
    "    \"You are a geographer describing river formations.\"\n",
    "]\n",
    "\n",
    "for context in contexts:\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Clarified response: {resolve_ambiguity(ambiguous_prompt, context)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques for Writing Clearer Prompts\n",
    "\n",
    "Let's explore some techniques for writing clearer prompts to improve AI responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prompt Response:\n",
      "It seems like you're asking for instructions on making something, but you haven't specified what that something is. Could you please provide more details or clarify what you're trying to make? That way, I can give you more accurate and helpful guidance.\n",
      "\n",
      "Improved Prompt Response:\n",
      "The classic margherita pizza, a timeless Italian favorite that never fails to impress. Here's a step-by-step guide to making a delicious margherita pizza at home:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "For the dough:\n",
      "\n",
      "* 1 1/2 cups warm water\n",
      "* 1 tablespoon sugar\n",
      "* 2 teaspoons active dry yeast\n",
      "* 3 1/2 cups all-purpose flour\n",
      "* 1 teaspoon salt\n",
      "* 2 tablespoons olive oil\n",
      "\n",
      "For the sauce:\n",
      "\n",
      "* 2 cups San Marzano tomatoes, crushed by hand\n",
      "* 2 cloves garlic, minced\n",
      "* 1 tablespoon olive oil\n",
      "* Salt, to taste\n",
      "\n",
      "For the toppings:\n",
      "\n",
      "* 8 ounces fresh mozzarella cheese, sliced\n",
      "* Fresh basil leaves, chopped\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "**Step 1: Make the Dough (1 hour 30 minutes)**\n",
      "\n",
      "1. In a large mixing bowl, combine the warm water, sugar, and yeast. Let the mixture sit for 5-10 minutes, or until the yeast is activated and foamy.\n",
      "2. Add the flour, salt, and olive oil to the bowl. Mix the dough using a wooden spoon or a stand mixer until it comes together in a shaggy mass.\n",
      "3. Knead the dough on a floured surface for 5-10 minutes, or until it becomes smooth and elastic.\n",
      "4. Place the dough in a lightly oiled bowl, cover it with plastic wrap, and let it rise in a warm, draft-free place for 1 hour, or until it has doubled in size.\n",
      "\n",
      "**Step 2: Prepare the Sauce (10 minutes)**\n",
      "\n",
      "1. In a small bowl, combine the crushed San Marzano tomatoes, garlic, and olive oil.\n",
      "2. Season the sauce with salt to taste.\n",
      "3. Set the sauce aside to use later.\n",
      "\n",
      "**Step 3: Shape the Dough (10 minutes)**\n",
      "\n",
      "1. Preheat your oven to 500°F (260°C) with a pizza stone or baking sheet inside. If you don't have a pizza stone, you can use a regular baking sheet.\n",
      "2. Punch down the risen dough and divide it into 2-4 equal portions, depending on the size of pizza you prefer.\n",
      "3. Roll out each portion into a thin circle, about 12 inches in diameter.\n",
      "4. Transfer the dough to a piece of parchment paper or a lightly floured pizza peel, if you have one.\n",
      "\n",
      "**Step 4: Top the Pizza (5 minutes)**\n",
      "\n",
      "1. Spread a thin\n"
     ]
    }
   ],
   "source": [
    "def compare_prompt_clarity(original_prompt, improved_prompt):\n",
    "    \"\"\"\n",
    "    Compare the responses to an original prompt and an improved, clearer version.\n",
    "    \"\"\"\n",
    "    original_response = llm.invoke(original_prompt).content\n",
    "    improved_response = llm.invoke(improved_prompt).content\n",
    "    return original_response, improved_response\n",
    "\n",
    "# Example usage\n",
    "original_prompt = \"How do I make it?\"\n",
    "improved_prompt = \"Provide a step-by-step guide for making a classic margherita pizza, including ingredients and cooking instructions.\"\n",
    "\n",
    "original_response, improved_response = compare_prompt_clarity(original_prompt, improved_prompt)\n",
    "\n",
    "print(\"Original Prompt Response:\")\n",
    "print(original_response)\n",
    "print(\"\\nImproved Prompt Response:\")\n",
    "print(improved_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Prompts for Clarity\n",
    "\n",
    "Using structured prompts can significantly improve clarity and consistency in AI responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert writing assistant.\n",
      "Summarize the following text in 3 bullet points:\n",
      "\n",
      "LangChain makes it easier to build LLM-powered apps.\n",
      "\n",
      "Model output:\n",
      " Here are three bullet points summarizing the text:\n",
      "\n",
      "* LangChain is a tool designed to simplify the development of applications.\n",
      "* It specifically focuses on building apps that utilize Large Language Models (LLMs).\n",
      "* By using LangChain, developers can more easily create and integrate LLM-powered functionality into their applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a template with placeholders\n",
    "template = \"\"\"\n",
    "You are an expert writing assistant.\n",
    "Summarize the following text in 3 bullet points:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # variables you’ll pass in\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Fill in the template\n",
    "formatted_prompt = prompt.format(\n",
    "    text=\"LangChain makes it easier to build LLM-powered apps.\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)\n",
    "\n",
    "# 4. Send to LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# 5. Print model’s answer\n",
    "print(\"Model output:\\n\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise: Improving Prompt Clarity\n",
    "\n",
    "Now, let's practice improving the clarity of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What's the difference?\n",
      "Improved: What is the difference between two specific things, such as ideas, concepts, objects, or situations, and how do their characteristics, functions, or outcomes compare and contrast with one another?\n",
      "--------------------------------------------------\n",
      "Original: How does it work?\n",
      "Improved: What are the specific mechanisms or processes by which this particular system, technology, or concept operates, and how do its various components interact to produce the desired outcome?\n",
      "--------------------------------------------------\n",
      "Original: Why is it important?\n",
      "Improved: What are the significant benefits or consequences of this issue, and how does it impact individuals, communities, or society as a whole?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "unclear_prompts = [\n",
    "    \"What's the difference?\",\n",
    "    \"How does it work?\",\n",
    "    \"Why is it important?\"\n",
    "]\n",
    "\n",
    "def improve_prompt_clarity(unclear_prompt):\n",
    "    \"\"\"\n",
    "    Improve the clarity of a given prompt.\n",
    "    \n",
    "    Args:\n",
    "    unclear_prompt (str): The original unclear prompt\n",
    "    \n",
    "    Returns:\n",
    "    str: An improved, clearer version of the prompt\n",
    "    \"\"\"\n",
    "    improvement_prompt = f\"The following prompt is unclear: '{unclear_prompt}'. Please provide a clearer, more specific version of this prompt. output just the improved prompt and nothing else.\" \n",
    "    return llm.invoke(improvement_prompt).content\n",
    "\n",
    "for prompt in unclear_prompts:\n",
    "    improved_prompt = improve_prompt_clarity(prompt)\n",
    "    print(f\"Original: {prompt}\")\n",
    "    print(f\"Improved: {improved_prompt}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
